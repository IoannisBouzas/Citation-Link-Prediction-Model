{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"yQuHJ7082kQD"},"outputs":[],"source":["import warnings\n","warnings.filterwarnings(\"ignore\")"]},{"cell_type":"markdown","source":["Install necessary libraries"],"metadata":{"id":"SJhnLDWCeCMn"}},{"cell_type":"code","source":["pip install igraph"],"metadata":{"id":"3IYaBmqdtx9_"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mdTdic6MDHwU"},"outputs":[],"source":["pip install -U gensim"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wIQxj6CV3fJm"},"outputs":[],"source":["pip install codecarbon"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0Cv5Nplu55ua"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","source":["If you want to use the codecarbon uncomment the below code"],"metadata":{"id":"Z6nIgIane9kc"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"jENo5P763D2i"},"outputs":[],"source":["# from codecarbon import EmissionsTracker\n","# tracker = EmissionsTracker()\n","# tracker.start()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AksQstx-CzKB"},"outputs":[],"source":["import re\n","import nltk\n","import torch\n","import random\n","import numpy as np\n","import networkx as nx\n","import matplotlib.pyplot as plt\n","import pandas as pd\n","import seaborn as sns\n","import spacy\n","import igraph as ig\n","import pickle\n","from tqdm import tqdm\n","from collections import defaultdict\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.model_selection import train_test_split\n","from gensim.parsing.preprocessing import remove_stopwords\n","from collections import Counter\n","from nltk.tokenize import word_tokenize\n","from nltk.stem import WordNetLemmatizer\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from multiprocessing import Pool\n","\n","\n","nltk.download('wordnet')\n","nltk.download('omw-1.4')\n","nltk.download('punkt_tab')\n","nltk.download('punkt')\n","nltk.download('stopwords')"]},{"cell_type":"markdown","source":["**Loading Data Phase**"],"metadata":{"id":"tWiZSlqDkDPP"}},{"cell_type":"markdown","source":["Load Every Data That We Have"],"metadata":{"id":"DXDubCshfNLN"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"SyzcH1yJteCH"},"outputs":[],"source":["# Load abstracts\n","abstracts = {}\n","with open(\"/content/drive/MyDrive/llms/abstracts.txt\", \"r\", encoding=\"utf-8\") as f:\n","    content = f.read()\n","    abstract_entries = content.split(\"\\n\")\n","\n","    for entry in abstract_entries:\n","        if entry and \"|--|\" in entry:\n","            parts = entry.split(\"|--|\")\n","            if len(parts) == 2:\n","                paper_id = int(parts[0])\n","                abstract_text = parts[1]\n","                abstracts[paper_id] = abstract_text\n","    print(f\"Loaded {len(abstracts)} abstracts.\")\n","\n","\n","\n","# Load citation network\n","edges = []\n","with open(\"/content/drive/MyDrive/llms/edgelist.txt\", \"r\") as f:\n","    for line in f:\n","        source, target = map(int, line.strip().split(\",\"))\n","        edges.append((source, target))\n","\n","# Create a directed graph\n","G = nx.DiGraph()\n","G.add_edges_from(edges)\n","# print(G.number_of_nodes())\n","# print(G.number_of_edges())\n","\n","\n","# Load authors\n","authors = {}\n","with open(\"/content/drive/MyDrive/llms/authors.txt\", \"r\", encoding=\"utf-8\") as f:\n","    for line in f:\n","        if \"|--|\" in line:\n","            parts = line.strip().split(\"|--|\")\n","            paper_id = int(parts[0])\n","            author_list = parts[1].split(\",\")\n","            authors[paper_id] = author_list\n","    print(f\"Loaded {len(authors)} authors.\")\n","\n"]},{"cell_type":"markdown","source":["Test Pairs"],"metadata":{"id":"w-GEOBclgPGs"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"8YEoJf0ii6-U"},"outputs":[],"source":["# # Load test pairs\n","test_pairs_kaggle = []\n","with open(\"/content/drive/MyDrive/llms/test.txt\", \"r\") as f:\n","    for line in f:\n","        source, target = map(int, line.strip().split(\",\"))\n","        test_pairs_kaggle.append((source, target))"]},{"cell_type":"markdown","source":["Node2Vec Embeddings"],"metadata":{"id":"omIVQPvkgR_e"}},{"cell_type":"code","source":["# Define the filename where the embeddings were saved\n","embeddings_filename = \"/content/drive/MyDrive/llms/node_embeddings.pkl\"\n","\n","# Load the dictionary from the file\n","with open(embeddings_filename, 'rb') as f:\n","    node_embeddings = pickle.load(f)"],"metadata":{"id":"L5uI6LAUrkaM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["all-MiniLM-L6-v2 Embedding(Sentence Transformer)"],"metadata":{"id":"cok0HYfYgtLZ"}},{"cell_type":"code","source":["sentence_bert_embeddings = np.load('/content/drive/MyDrive/llms/all-MiniLM-L6-v2.npy', allow_pickle=True).item()"],"metadata":{"id":"_NkJtEa3oOLf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["all-mpnet-base-v2 Embedding(Sentence Transformer)"],"metadata":{"id":"U_26RwLJgy1K"}},{"cell_type":"code","source":["sentence_bert_embeddings = np.load('/content/drive/MyDrive/llms/all-mpnet-base-v2.npy', allow_pickle=True).item()"],"metadata":{"id":"pkwBCPqJnZSA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["SciBERT Embeddings"],"metadata":{"id":"97J7Hnjcg4Qf"}},{"cell_type":"code","source":["sci_bert = np.load('/content/drive/MyDrive/llms/scibert_embeddings.npy', allow_pickle=True).item()"],"metadata":{"id":"5T8HzMT2f7eU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Specter Embeddings"],"metadata":{"id":"T9LSy0j4g42b"}},{"cell_type":"code","source":["specter_bert = np.load('/content/drive/MyDrive/llms/specter_embeddings.npy', allow_pickle=True).item()"],"metadata":{"id":"wZdIeuUngbcd"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["DistilBERT Embeddings"],"metadata":{"id":"7RIOLDi8g5J-"}},{"cell_type":"code","source":["distill_bert = np.load('/content/drive/MyDrive/llms/distilbert_embeddings.npy', allow_pickle=True).item()"],"metadata":{"id":"2zGWdGdOgXCc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Here we will split our data into train,val and test in order to have propel splitting and avoid data leakage"],"metadata":{"id":"LHidEoa2h1Yt"}},{"cell_type":"code","source":["random_state=42\n","random.seed(random_state)\n","np.random.seed(random_state)\n","\n","test_ratio=0.80\n","val_ratio=0.10\n","test_ratio = 0.10\n","\n","# Get all edges from original graph\n","all_edges = list(G.edges())\n","all_nodes = list(G.nodes())\n","\n","print(f\"Original graph - Nodes: {len(all_nodes)}, Edges: {len(all_edges)}\")\n","\n","# Split edges into test and train_val\n","train_val_edges, test_edges = train_test_split(\n","    all_edges,\n","    test_size=test_ratio,\n","    random_state=random_state\n",")\n","\n","# Do this in order to have 80% training edges, 10% val and 10% test\n","val_size_relative_to_train_val = val_ratio / (1.0 - test_ratio)\n","\n","train_edges, val_edges = train_test_split(\n","    train_val_edges,\n","    test_size=val_size_relative_to_train_val,\n","    random_state=random_state\n",")\n","\n","print(f\"Train edges: {len(train_edges)} ({len(train_edges)/len(all_edges)*100:.1f}%)\")\n","print(f\"Val edges: {len(val_edges)} ({len(val_edges)/len(all_edges)*100:.1f}%)\")\n","print(f\"Test edges: {len(test_edges)} ({len(test_edges)/len(all_edges)*100:.1f}%)\")\n","\n","# Create training graph\n","G_train = nx.DiGraph() if G.is_directed() else nx.Graph()\n","G_train.add_edges_from(train_edges)\n","G_train.add_nodes_from(all_nodes)\n","\n","print(f\"Training graph - Nodes: {G_train.number_of_nodes()}, Edges: {G_train.number_of_edges()}\")"],"metadata":{"id":"ux6vKPhBTKOx"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Here we generate negative samples for every train_pairs, val_pairs, test_pairs.\n","The method is simple. Every time the function iteratively picks random pairs of nodes, checks if they represent an existing edge in the training graph or any positive edge in the original graph, or if they have already been generated as negatives. If none of these conditions are met, the pair is added to the list of negative samples until the desired number is reached or a maximum number of attempts is exceeded."],"metadata":{"id":"Q61iyksUiFpJ"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"TBVdwTZ9QxhF"},"outputs":[],"source":["def generate_negative_samples(graph, positive_edges_all, existing_negatives, n_samples, seed=42):\n","    print(f\"Generating {n_samples} negative samples for directed graph...\")\n","    random.seed(seed)\n","    nodes = list(graph.nodes())\n","    existing_edges = set(graph.edges())\n","    all_positive = set(positive_edges_all)\n","\n","    negative_pairs = set()\n","    max_attempts = n_samples * 20\n","    attempts = 0\n","\n","    with tqdm(total=n_samples) as pbar:\n","        while len(negative_pairs) < n_samples and attempts < max_attempts:\n","            u, v = random.sample(nodes, 2)\n","            edge = (u, v)\n","\n","            if (edge not in existing_edges and\n","                edge not in all_positive and\n","                edge not in existing_negatives and\n","                edge not in negative_pairs):\n","                negative_pairs.add(edge)\n","                pbar.update(1)\n","            attempts += 1\n","\n","    if len(negative_pairs) < n_samples:\n","        print(f\"Only generated {len(negative_pairs)} negative samples out of requested {n_samples}.\")\n","\n","    return list(negative_pairs)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4dGD0Hcxgv6n"},"outputs":[],"source":["# Generate negative samples sequentially to avoid overlap\n","positive_edges_all = set(train_edges + val_edges + test_edges)\n","\n","# Generate train negatives\n","train_negative = generate_negative_samples(\n","    G_train, positive_edges_all, set(), len(train_edges), seed=42\n",")\n","\n","# Generate val negatives\n","val_negative = generate_negative_samples(\n","    G_train, positive_edges_all, set(train_negative), len(val_edges), seed=43\n",")\n","\n","# Generate test negatives\n","test_negative = generate_negative_samples(\n","    G_train, positive_edges_all, set(train_negative + val_negative), len(test_edges), seed=44\n",")"]},{"cell_type":"markdown","source":["Here we have properly created our data into train, val and test"],"metadata":{"id":"kaHcVRgijX3T"}},{"cell_type":"code","source":["train_pairs = train_edges + train_negative\n","train_labels = [1] * len(train_edges) + [0] * len(train_negative)\n","\n","val_pairs = val_edges + val_negative\n","val_labels = [1] * len(val_edges) + [0] * len(val_negative)\n","\n","test_pairs = test_edges + test_negative\n","test_labels = [1] * len(test_edges) + [0] * len(test_negative)"],"metadata":{"id":"sVluDCu0SpAK"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Preprocessing Phase**"],"metadata":{"id":"1ff1uQVTjyin"}},{"cell_type":"markdown","source":["Here we do basic NLP techniques to preprocess our abstracts"],"metadata":{"id":"ijQ5nOiyj3DZ"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"5OntCpUymgdA"},"outputs":[],"source":["# Preprocess abstracts\n","lemmatizer = WordNetLemmatizer()\n","\n","\n","\n","def preprocess_text(text):\n","    # Remove special chars & lowercase\n","    text = re.sub(r'[^a-zA-Z\\s]', '', text.lower())\n","\n","    # Remove stopwords\n","    text = remove_stopwords(text)\n","\n","\n","    tokens = word_tokenize(text)\n","\n","    # create stemmer object\n","    stemmer = nltk.stem.PorterStemmer()\n","\n","    # stem each token\n","    stemmed_tokens = [stemmer.stem(token) for token in tokens]\n","\n","    # # Lemmatize\n","    # tokens = word_tokenize(stemmed_tokens)\n","    # lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]\n","\n","    return ' '.join(stemmed_tokens)\n","\n","\n","\n","def preprocess_batch(batch):\n","    results = {}\n","    for paper_id, abstract in batch:\n","        results[paper_id] = preprocess_text(abstract)\n","    return results\n","\n","\n","\n","# Preprocess all abstracts using batches and multiprocessing\n","batch_size = 10000\n","num_processes = 12\n","\n","with Pool(processes=num_processes) as pool:\n","    batches = [list(abstracts.items())[i:i + batch_size] for i in range(0, len(abstracts), batch_size)]\n","    results = pool.map(preprocess_batch, batches)\n","\n","\n","# Combine the results from all batches\n","preprocessed_abstracts = {}\n","for batch_result in results:\n","    preprocessed_abstracts.update(batch_result)\n","\n","# Print the 5 first rows to see how the Preprocess abstracts are\n","df = pd.DataFrame(list(preprocessed_abstracts.items()), columns=['paper_id', 'abstract'])\n","\n","print(df.head())\n","\n","\n"]},{"cell_type":"markdown","source":["**Features Engineering**"],"metadata":{"id":"gUItOey4kgD_"}},{"cell_type":"markdown","source":["**TF-IDF**: Tells you how important a single word is in a corpus\n","by assigning it a weight, and at the same time down-weight common words like “a”, “and”, and “the”"],"metadata":{"id":"0OGiotBikjox"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"MMRhiT17wnfc"},"outputs":[],"source":["corpus = list(preprocessed_abstracts.values())\n","\n","\n","# features 20000, ngram_range specifies the boundary of this range which will be extracted from the corpus\n","vectorizer = TfidfVectorizer(max_features=20000,ngram_range=(1, 2))\n","tfidf_matrix = vectorizer.fit_transform(corpus)\n","\n","\n","# Map paper IDs to their TF-IDF vectors\n","paper_to_tfidf = {paper_id: tfidf_matrix[i] for i, paper_id in enumerate(preprocessed_abstracts.keys())}\n","\n","# Convert sparse vectors to dense vectors once\n","paper_to_dense_tfidf = {\n","    paper_id: tfidf_matrix[i].toarray().flatten()\n","    for i, paper_id in enumerate(preprocessed_abstracts.keys())\n","}\n","\n","# Precompute norms once\n","paper_to_norm = {\n","    paper_id: np.linalg.norm(vec)\n","    for paper_id, vec in paper_to_dense_tfidf.items()\n","}\n","\n","\n"]},{"cell_type":"markdown","source":["**get_text_similarity**: Quantifies how similar the abstracts of two papers are based on the weighted importance of the words they contain"],"metadata":{"id":"EgiDiv4llkIs"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"Gov8dj6ix1Zt"},"outputs":[],"source":["def get_text_similarity(paper1, paper2):\n","    if paper1 not in paper_to_dense_tfidf or paper2 not in paper_to_dense_tfidf:\n","        return 0.0\n","    vec1 = paper_to_dense_tfidf[paper1]\n","    vec2 = paper_to_dense_tfidf[paper2]\n","    norm1 = paper_to_norm[paper1]\n","    norm2 = paper_to_norm[paper2]\n","    denominator = norm1 * norm2 + 1e-8\n","    if denominator == 0:\n","        return 0.0\n","    return np.dot(vec1, vec2) / denominator"]},{"cell_type":"markdown","source":["**get_author_overlap** : It measures the similarity between the authors of two papers"],"metadata":{"id":"x_w-8-PfmJgl"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"09g65Fw0wn1D"},"outputs":[],"source":["def get_author_overlap(paper1, paper2):\n","    if paper1 not in authors or paper2 not in authors:\n","        return 0.0\n","    authors1 = set(authors[paper1])\n","    authors2 = set(authors[paper2])\n","    overlap = len(authors1.intersection(authors2))\n","    union = len(authors1.union(authors2))\n","    return overlap / union if union > 0 else 0.0"]},{"cell_type":"markdown","source":["**get_common_neighbors**: This function calculates the number of common \"out-neighbors\" between two papers in the training graph"],"metadata":{"id":"lzHHK6JemnOR"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"tx5zm7kz8WHR"},"outputs":[],"source":["def get_common_neighbors(paper1, paper2):\n","    if paper1 not in G_train.nodes or paper2 not in G_train.nodes:\n","        return 0\n","    neighbors1 = set(G_train.successors(paper1))\n","    neighbors2 = set(G_train.successors(paper2))\n","    return len(neighbors1.intersection(neighbors2))"]},{"cell_type":"markdown","source":["**get_jaccard_coefficient**: It measures the similarity between the sets of papers cited by paper1 and paper2"],"metadata":{"id":"98FAlI3jm1YO"}},{"cell_type":"code","source":["def get_jaccard_coefficient(paper1, paper2):\n","    if paper1 not in G_train.nodes or paper2 not in G_train.nodes:\n","        return 0.0\n","    neighbors1 = set(G_train.successors(paper1))\n","    neighbors2 = set(G_train.successors(paper2))\n","    intersection = len(neighbors1.intersection(neighbors2))\n","    union = len(neighbors1.union(neighbors2))\n","    return intersection / union if union > 0 else 0.0"],"metadata":{"id":"6sVfR4kVl_U_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**get_preferential_attachment**: This function calculates the preferential attachment score between two papers based on their out-degrees in the training graph"],"metadata":{"id":"KwAewGnVm1x4"}},{"cell_type":"code","source":["def get_preferential_attachment(paper1, paper2):\n","    if paper1 not in G_train.nodes or paper2 not in G_train.nodes:\n","        return 0\n","    return G_train.out_degree(paper1) * G_train.out_degree(paper2)"],"metadata":{"id":"eDe6eiwSmCe9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**get_embedding_similarity** : This function calculates the cosine similarity between the vector representations (embeddings) of two nodes using **Node2Vec**"],"metadata":{"id":"xDFhfK0NoCRv"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"aTQqRA914SY1"},"outputs":[],"source":["def get_embedding_similarity(source, target, node_embeddings):\n","    \"\"\"\n","    Calculate cosine similarity between node embeddings with better error handling\n","    \"\"\"\n","    if source not in node_embeddings or target not in node_embeddings:\n","        return 0.0\n","\n","    source_emb = node_embeddings[source]\n","    target_emb = node_embeddings[target]\n","\n","    dot_product = np.dot(source_emb, target_emb)\n","    norm_source = np.linalg.norm(source_emb)\n","    norm_target = np.linalg.norm(target_emb)\n","\n","    if norm_source * norm_target == 0:\n","        return 0.0\n","    return dot_product / (norm_source * norm_target)"]},{"cell_type":"markdown","source":["**calculate_adamic_adar** : This function calculates the Adamic-Adar index between two nodes (source and target) based on their common neighbors in the training graph (G_train)"],"metadata":{"id":"zahkgkpooHka"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"c1dfkuKCH5TD"},"outputs":[],"source":["import numpy as np\n","import networkx as nx\n","from collections import defaultdict\n","\n","\n","adamic_adar_cache = {}\n","\n","def calculate_adamic_adar(source, target):\n","    if (source, target) in adamic_adar_cache:\n","        return adamic_adar_cache[(source, target)]\n","\n","    adamic_adar = 0\n","    try:\n","        common_neighbors_set = set(G_train.neighbors(source)).intersection(set(G_train.neighbors(target)))\n","        adamic_adar = sum(1/np.log(1 + G_train.degree(n)) for n in common_neighbors_set)\n","    except:\n","        pass\n","    adamic_adar_cache[(source, target)] = adamic_adar\n","    return adamic_adar\n"]},{"cell_type":"markdown","source":["**calculate_shortest_path** : This function calculates a feature related to the shortest path length between two nodes (source and target) in the training graph"],"metadata":{"id":"ez101xXvoLPI"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"2y2R71iWMONr"},"outputs":[],"source":["shortest_path_cache = {}\n","\n","def calculate_shortest_path(source, target):\n","  if (source, target) in shortest_path_cache:\n","      return shortest_path_cache[(source, target)]\n","\n","  try:\n","      path_length = nx.shortest_path_length(G_train, source=source, target=target)\n","      shortest_path = 1.0 / (path_length + 1)\n","  except:\n","      shortest_path = 0\n","\n","  shortest_path_cache[(source, target)] = shortest_path\n","  return shortest_path"]},{"cell_type":"markdown","source":["**get_sentence_bert_features** : We calculate with the help of the sentece models the maximum similarity score (indicating the most similar sentence pair), the average similarity across all sentence pairs (providing an overall semantic relatedness measure), and the count of sentence pairs exceeding a specified similarity threshold (capturing the volume of highly similar content"],"metadata":{"id":"l2T787heoNdr"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"LXMekkVMlFnQ"},"outputs":[],"source":["def get_sentence_bert_features(paper1, paper2, sentence_bert_embeddings, threshold=0.8):\n","\n","    if paper1 not in sentence_bert_embeddings or paper2 not in sentence_bert_embeddings:\n","        return 0.0, 0.0, 0\n","\n","    emb1 = sentence_bert_embeddings[paper1]\n","    emb2 = sentence_bert_embeddings[paper2]\n","\n","    sims = []\n","    for vec1 in emb1:\n","        for vec2 in emb2:\n","            sim = np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2) + 1e-8)\n","            sims.append(sim)\n","\n","    if not sims:\n","        return [0.0, 0.0, 0]\n","\n","    avg_sim = np.mean(sims)\n","    max_sim = np.max(sims)\n","    min_sim = np.min(sims)\n","    count_above = sum(s > threshold for s in sims)\n","\n","    return max_sim, avg_sim, count_above\n"]},{"cell_type":"markdown","source":["**get_bert_similarity** :  This function calculates the cosine similarity between the embedding vectors generated by a BERT-based model (like SciBERT, Specter, or DistilBERT)"],"metadata":{"id":"thIOs541oQWh"}},{"cell_type":"code","source":["def get_bert_similarity(source, target, bert_embeddings):\n","\n","    # Handle cases where embeddings are not found\n","    if source_emb is None or target_emb is None:\n","        return 0.0\n","\n","    # Ensure embeddings are NumPy arrays\n","    source_emb = np.asarray(source_emb)\n","    target_emb = np.asarray(target_emb)\n","\n","    # Handle cases where embeddings are zero vectors\n","    if np.all(source_emb == 0) or np.all(target_emb == 0):\n","        return 0.0\n","\n","    dot_product = np.dot(source_emb, target_emb)\n","    norm_source = np.linalg.norm(source_emb)\n","    norm_target = np.linalg.norm(target_emb)\n","\n","    if norm_source * norm_target == 0:\n","        return 0.0\n","    return dot_product / (norm_source * norm_target)"],"metadata":{"id":"qELg7fw_p2mG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We calculate graph attributes like Clustering Coefficient , PageRank , •\tHITS Algorithm , Betweenness Centrality"],"metadata":{"id":"ad-Jicm6q4Sb"}},{"cell_type":"code","source":["ig_G = ig.Graph.from_networkx(G_train)\n","\n","cluster = nx.clustering(G_train)\n","\n","rank = nx.pagerank(G_train)\n","\n","h,a = nx.hits(G_train)\n","\n","bet = ig_G.betweenness(directed=True, cutoff=5)\n","\n","katz_centrality = nx.katz_centrality(G_train)"],"metadata":{"id":"cP9bW6VJnr6G"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Here we generate the training data for the negatives and positives examples . Here the user can uncomment or comment out the features that want to have the models or the features that he does not want to have"],"metadata":{"id":"R5tpTsXjrSSN"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"s5iLMFppQ18U"},"outputs":[],"source":["import numpy as np\n","import networkx as nx\n","from collections import defaultdict\n","from tqdm import tqdm\n","import time\n","\n","def generate_training_data(negatives, positives):\n","\n","\n","    X_data = []\n","    y_data = []\n","\n","    # Process NEGATIVE examples\n","    for source, target in tqdm(negatives, desc=\"Processing negative examples\"):\n","        #start_time = time.time()\n","\n","        # max_sim, avg_sim, count_above = get_sentence_bert_features(source, target, sentence_bert_embeddings)\n","\n","\n","        # Combine original features with new ones\n","        features = [\n","            get_text_similarity(source, target),\n","            get_author_overlap(source, target),\n","            get_common_neighbors(source, target),\n","            get_jaccard_coefficient(source, target),\n","            get_preferential_attachment(source, target),\n","            get_embedding_similarity(source, target, node_embeddings),\n","            get_bert_similarity(source, target, sci_bert),\n","            cluster[source] + cluster[target],\n","            abs(cluster[source] - cluster[target]),\n","            rank[source] + rank[target],\n","            abs(rank[source] - rank[target]),\n","            h[source] + h[target],\n","            abs(h[source] - h[target]),\n","            a[source] + a[target],\n","            abs(a[source] - a[target]),\n","            bet[source] + bet[target],\n","            abs(bet[source] - bet[target]),\n","            katz_centrality[source] + katz_centrality[target],\n","            abs(katz_centrality[source] - katz_centrality[target]),\n","            calculate_adamic_adar(source, target),\n","            # calculate_shortest_path(source, target)\n","            # max_sim,\n","            # avg_sim,\n","            # count_above,\n","        ]\n","\n","\n","\n","        #end_time = time.time()\n","        X_data.append(features)\n","        y_data.append(0)\n","\n","\n","\n","\n","    # Process POSITIVE examples\n","    for source, target in tqdm(positives, desc=\"Processing positive examples\"):\n","        #start_time = time.time()  # Start timer\n","\n","        # max_sim, avg_sim, count_above = get_sentence_bert_features(source, target, sentence_bert_embeddings)\n","\n","\n","        # Combine original features with new ones\n","        features = [\n","            get_text_similarity(source, target),\n","            get_author_overlap(source, target),\n","            get_common_neighbors(source, target),\n","            get_jaccard_coefficient(source, target),\n","            get_preferential_attachment(source, target),\n","            get_embedding_similarity(source, target, node_embeddings),\n","            get_bert_similarity(source, target, sci_bert),\n","            cluster[source] + cluster[target],\n","            abs(cluster[source] - cluster[target]),\n","            rank[source] + rank[target],\n","            abs(rank[source] - rank[target]),\n","            h[source] + h[target],\n","            abs(h[source] - h[target]),\n","            a[source] + a[target],\n","            abs(a[source] - a[target]),\n","            bet[source] + bet[target],\n","            abs(bet[source] - bet[target]),\n","            katz_centrality[source] + katz_centrality[target],\n","            abs(katz_centrality[source] - katz_centrality[target]),\n","            calculate_adamic_adar(source, target),\n","            # calculate_shortest_path(source, target)\n","            # max_sim,\n","            # avg_sim,\n","            # count_above,\n","\n","        ]\n","\n","        #end_time = time.time()\n","        X_data.append(features)\n","        y_data.append(1)  # Positive example\n","\n","\n","\n","    return np.array(X_data), np.array(y_data)"]},{"cell_type":"markdown","source":["**X_train, y_train sets**"],"metadata":{"id":"wCDzU0xprnTG"}},{"cell_type":"code","source":["X_train, y_train = generate_training_data(train_negative, train_edges)"],"metadata":{"id":"r-Z9t4NaiQJ4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**X_val, y_val sets**"],"metadata":{"id":"raBxAF2orr_R"}},{"cell_type":"code","source":["X_val, y_val = generate_training_data(val_negative, val_edges)"],"metadata":{"id":"0gkwhUoli3ut"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**X_test, y_test sets**"],"metadata":{"id":"Ja08WhxfrsSy"}},{"cell_type":"code","source":["X_test, y_test = generate_training_data(test_negative, test_edges)"],"metadata":{"id":"b_wi_X1vBCWd"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"guZoxXVz0aIL"},"outputs":[],"source":["# print the shape of our data\n","print(X_train.shape)\n","print(X_val.shape)\n","print(X_test.shape)"]},{"cell_type":"markdown","source":["**Scale the Features to normalize the range, distribution, and magnitude of features, reducing potential biases and inconsistencies that may arise from variations in their values**"],"metadata":{"id":"B8jbxGIvr3US"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZRSzLdEVAud0"},"outputs":[],"source":["# Scale features\n","scaler = StandardScaler()\n","X_train_scaled = scaler.fit_transform(X_train)\n","X_val_scaled = scaler.transform(X_val)\n","X_test_scaled = scaler.transform(X_test)"]},{"cell_type":"markdown","source":["**MODELS**"],"metadata":{"id":"re1r4_Lzr_8V"}},{"cell_type":"markdown","source":["Here we have our different models and architectures and some statistics for every model like **accuracy_score, precision_score, recall_score, f1_score**"],"metadata":{"id":"wnGoiQ9EsCT7"}},{"cell_type":"markdown","source":["More specific we have XGBOOST where we tested many parameters"],"metadata":{"id":"Zs319uKJsGmH"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"VuM871gNgUDB"},"outputs":[],"source":["!pip install xgboost"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_D-3O9pDsHX5"},"outputs":[],"source":["from sklearn.model_selection import cross_val_score, StratifiedKFold\n","from xgboost import XGBClassifier\n","from sklearn.metrics import log_loss,accuracy_score, precision_score, recall_score, f1_score"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"e4vO-Epb8XnG"},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","from xgboost import XGBClassifier\n","from sklearn.model_selection import StratifiedKFold, cross_val_score\n","from sklearn.metrics import (log_loss, roc_curve, auc, precision_recall_curve,\n","                           average_precision_score, confusion_matrix,\n","                           ConfusionMatrixDisplay, classification_report,\n","                           accuracy_score, precision_score, recall_score, f1_score)\n","\n","#############################################\n","################ XGBOOST #####################\n","\n","# # XGBoost Classifier with parameters\n","bst = XGBClassifier(\n","    n_estimators=100,\n","    max_depth=4,              # Reduced complexity\n","    learning_rate=0.05,       # Slower learning\n","    min_child_weight=3,       # More regularization\n","    gamma=0.1,\n","    subsample=0.8,\n","    colsample_bytree=0.8,\n","    reg_alpha=0.1,            # L1 regularization\n","    reg_lambda=1.0,           # L2 regularization\n","    random_state=42,\n","    eval_metric='logloss'\n",")\n","\n","# # Slightly more conservative parameters\n","# bst = XGBClassifier(\n","#     n_estimators=80,           # Reduced slightly\n","#     max_depth=5,               # Moderate depth\n","#     learning_rate=0.08,        # Moderate learning rate\n","#     min_child_weight=2,\n","#     gamma=0.05,\n","#     subsample=0.85,\n","#     colsample_bytree=0.85,\n","#     reg_alpha=0.05,\n","#     reg_lambda=0.5,\n","#     random_state=42,\n","#     eval_metric='logloss'\n","# )\n","\n","# bst = XGBClassifier(\n","#     objective='binary:logistic',\n","#     eval_metric='logloss',\n","#     learning_rate=0.01,  # Much lower than default (0.3)\n","#     max_depth=4,         # Lower than default (6)\n","#     min_child_weight=10, # Higher than default (1)\n","#     subsample=0.8,       # Row sampling\n","#     colsample_bytree=0.8, # Feature sampling\n","#     reg_alpha=1.0,       # L1 regularization\n","#     reg_lambda=5.0,      # L2 regularization (higher for graph data)\n","#     scale_pos_weight=1,  # Adjust if class imbalance\n","#     random_state=42\n","# )\n","\n","\n","\n","print(\"=== XGBoost Link Prediction Model ===\\n\")\n","\n","# Cross-validation setup\n","print(\"Performing 10-fold cross-validation...\")\n","kf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n","\n","# Perform cross-validation focusing on log loss\n","cv_scores = cross_val_score(bst, X_train_scaled, y_train, cv=kf, scoring='neg_log_loss', verbose=1)\n","\n","print(f\"Cross-validation scores (neg_log_loss): {cv_scores}\")\n","print(f\"Mean CV Log Loss: {-cv_scores.mean():.4f} (+/- {cv_scores.std() * 2:.4f})\")\n","\n","# Train the model\n","print(\"\\nTraining XGBoost model...\")\n","bst.fit(X_train_scaled, y_train)\n","\n","# Validation predictions\n","y_val_pred_proba = bst.predict_proba(X_val_scaled)[:, 1]\n","y_val_pred = bst.predict(X_val_scaled)\n","\n","# Calculate validation metrics\n","val_loss = log_loss(y_val, y_val_pred_proba)\n","val_accuracy = accuracy_score(y_val, y_val_pred)\n","val_precision = precision_score(y_val, y_val_pred)\n","val_recall = recall_score(y_val, y_val_pred)\n","val_f1 = f1_score(y_val, y_val_pred)\n","\n","print(f\"\\n=== Validation Results ===\")\n","print(f\"Validation Log Loss: {val_loss:.4f}\")\n","print(f\"Validation Accuracy: {val_accuracy:.4f}\")\n","print(f\"Validation Precision: {val_precision:.4f}\")\n","print(f\"Validation Recall: {val_recall:.4f}\")\n","print(f\"Validation F1-Score: {val_f1:.4f}\")\n","\n","\n","\n","# Generate all visualizations\n","print(f\"\\n=== Generating Visualizations ===\")\n","\n","\n","# Detailed classification report\n","print(f\"\\n=== Detailed Classification Report ===\")\n","print(classification_report(y_val, y_val_pred, target_names=['No Link', 'Link']))\n","\n","\n","# FINAL TEST EVALUATION\n","print(f\"\\n=== FINAL TEST EVALUATION ===\")\n","\n","\n","y_test_pred_proba = bst.predict_proba(X_test_scaled)[:, 1]\n","y_test_pred = bst.predict(X_test_scaled)\n","\n","test_loss = log_loss(y_test, y_test_pred_proba)\n","test_accuracy = accuracy_score(y_test, y_test_pred)\n","\n","print(f\"Final Test Log Loss: {test_loss:.4f}\")\n","print(f\"Final Test Accuracy: {test_accuracy:.4f}\")\n","\n","\n","print(f\"\\nModel training and validation complete!\")"]},{"cell_type":"markdown","source":["Also here we print the feature importance for our **XGB Models**"],"metadata":{"id":"cMiWFw8KtUM9"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ng553WTPBMZN"},"outputs":[],"source":["bst.feature_importances_"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"d4zlkeTXBg6I"},"outputs":[],"source":["from xgboost import plot_importance\n","import matplotlib.pyplot as plt\n","\n","bst.feature_importances_\n","\n","plot_importance(bst)\n","plt.show()"]},{"cell_type":"markdown","source":["Same for the **LogisticRegression Models**. This was our baseline model"],"metadata":{"id":"69eVBTyLtrN5"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"cNgd9WX1c2Y3"},"outputs":[],"source":["from sklearn.linear_model import LogisticRegression\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n","\n","\n","\n","model = LogisticRegression(max_iter=20000, random_state=42)\n","\n","\n","# K is 10 folds\n","kf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n","\n","# Perform cross-validation, focusing on log loss\n","cv_scores = cross_val_score(model, X_train_scaled, y_train, cv=kf, scoring='neg_log_loss')\n","\n","print(cv_scores)\n","\n","\n","model.fit(X_train_scaled, y_train)\n","y_pred_proba = model.predict_proba(X_val_scaled)[:, 1]\n","val_loss = log_loss(y_val, y_pred_proba)\n","print(\"K was 10 folds\\n\")\n","\n","\n","y_pred = model.predict(X_val_scaled)\n","\n","\n","# Calculate evaluation metrics\n","val_loss = log_loss(y_val, y_pred_proba)\n","accuracy = accuracy_score(y_val, y_pred)\n","precision = precision_score(y_val, y_pred)\n","recall = recall_score(y_val, y_pred)\n","f1 = f1_score(y_val, y_pred)\n","\n","print(f\"Logistic Regression - Validation Log Loss: {val_loss}\")\n","print(f\"Accuracy: {accuracy}, Precision: {precision}, Recall: {recall}, F1: {f1}\")\n","\n","\n","y_test_pred_proba = model.predict_proba(X_test_scaled)[:, 1]\n","y_test_pred = model.predict(X_test_scaled)\n","\n","test_loss = log_loss(y_test, y_test_pred_proba)\n","test_accuracy = accuracy_score(y_test, y_test_pred)\n","\n","print(f\"Final Test Log Loss: {test_loss:.4f}\")\n","print(f\"Final Test Accuracy: {test_accuracy:.4f}\")\n","\n","\n","print(f\"\\nModel training and validation complete!\")"]},{"cell_type":"markdown","source":["Our **LGB Models**"],"metadata":{"id":"jE5-SCtfuPU9"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"7lEDTbgRd7qQ"},"outputs":[],"source":["import lightgbm as lgb\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_curve, auc, precision_recall_curve, average_precision_score\n","\n","\n","lgb_model = lgb.LGBMClassifier(\n","    n_estimators=100,\n","    max_depth=4,              # Reduced complexity\n","    learning_rate=0.05,       # Slower learning\n","    min_child_weight=3,       # More regularization\n","    gamma=0.1,\n","    subsample=0.8,\n","    colsample_bytree=0.8,\n","    reg_alpha=0.1,            # L1 regularization\n","    reg_lambda=1.0,           # L2 regularization\n","    verbose=-1,\n","    random_state=42)\n","\n","\n","\n","\n","# K is 10 folds\n","kf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n","\n","# Perform cross-validation, focusing on log loss\n","cv_scores = cross_val_score(lgb_model, X_train_scaled, y_train, cv=kf, scoring='neg_log_loss')\n","\n","print(cv_scores)\n","\n","\n","lgb_model.fit(X_train_scaled, y_train)\n","y_pred_proba = lgb_model.predict_proba(X_val_scaled)[:, 1]\n","y_pred = lgb_model.predict(X_val_scaled)\n","\n","val_loss = log_loss(y_val, y_pred_proba)\n","print(\"K was 10 folds\\n\")\n","print(f\"LIGHTGBM Validation Log Loss: {val_loss}\")\n","\n","# Calculate evaluation metrics\n","val_loss = log_loss(y_val, y_pred_proba)\n","accuracy = accuracy_score(y_val, y_pred)\n","precision = precision_score(y_val, y_pred)\n","recall = recall_score(y_val, y_pred)\n","f1 = f1_score(y_val, y_pred)\n","\n","\n","print(f\"Accuracy: {accuracy}, Precision: {precision}, Recall: {recall}, F1: {f1}\")\n","\n","\n","# ROC Curve\n","fpr, tpr, thresholds = roc_curve(y_val, y_pred_proba)\n","roc_auc = auc(fpr, tpr)\n","\n","plt.figure(figsize=(8, 6))\n","plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\n","plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n","plt.xlim([0.0, 1.0])\n","plt.ylim([0.0, 1.05])\n","plt.xlabel('False Positive Rate')\n","plt.ylabel('True Positive Rate')\n","plt.title('Receiver Operating Characteristic (ROC) Curve')\n","plt.legend(loc=\"lower right\")\n","plt.show()\n","\n","# Precision-Recall Curve\n","precision, recall, _ = precision_recall_curve(y_val, y_pred_proba)\n","average_precision = average_precision_score(y_val, y_pred_proba)\n","\n","plt.figure(figsize=(8, 6))\n","plt.step(recall, precision, where='post', label='Precision-Recall curve (AP = %0.2f)' % average_precision)\n","plt.xlabel('Recall')\n","plt.ylabel('Precision')\n","plt.ylim([0.0, 1.05])\n","plt.xlim([0.0, 1.0])\n","plt.title('Precision-Recall Curve')\n","plt.legend(loc=\"lower left\")\n","plt.show()\n","\n"]},{"cell_type":"markdown","source":["Again here we print the feature importance for our **LGB Model**"],"metadata":{"id":"fGF5pH0buvel"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"mtsm1ETIzV5-"},"outputs":[],"source":["print(lgb_model.feature_importances_)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oibStaSaCvjV"},"outputs":[],"source":["import lightgbm as lgb\n","import matplotlib.pyplot as plt\n","\n","lgb.plot_importance(lgb_model, importance_type='split')\n","plt.title('Feature Importance')\n","plt.show()"]},{"cell_type":"markdown","source":["**A simle MLP Model with 2 layers**"],"metadata":{"id":"otKU7u9au2j1"}},{"cell_type":"code","source":["from sklearn.neural_network import MLPClassifier\n","from sklearn.preprocessing import StandardScaler\n","\n","\n","mlp = MLPClassifier(\n","    hidden_layer_sizes=(10, 5),\n","    activation='relu',\n","    alpha=0.01,  # L2 regularization\n","    learning_rate_init=0.001,\n","    max_iter=500,\n","    early_stopping=True,\n","    validation_fraction=0.1,\n","    n_iter_no_change=20,\n","    random_state=42\n",")\n","\n","mlp.fit(X_train_scaled, y_train)\n","\n","# Validation predictions\n","y_val_pred_proba = mlp.predict_proba(X_val_scaled)[:, 1]\n","y_val_pred = mlp.predict(X_val_scaled)\n","\n","# Calculate validation metrics\n","val_loss = log_loss(y_val, y_val_pred_proba)\n","val_accuracy = accuracy_score(y_val, y_val_pred)\n","val_precision = precision_score(y_val, y_val_pred)\n","val_recall = recall_score(y_val, y_val_pred)\n","val_f1 = f1_score(y_val, y_val_pred)\n","\n","print(f\"\\n=== Validation Results ===\")\n","print(f\"Validation Log Loss: {val_loss:.4f}\")\n","print(f\"Validation Accuracy: {val_accuracy:.4f}\")\n","print(f\"Validation Precision: {val_precision:.4f}\")\n","print(f\"Validation Recall: {val_recall:.4f}\")\n","print(f\"Validation F1-Score: {val_f1:.4f}\")\n","\n","\n","\n","# Generate all visualizations\n","print(f\"\\n=== Generating Visualizations ===\")\n","\n","\n","# Detailed classification report\n","print(f\"\\n=== Detailed Classification Report ===\")\n","print(classification_report(y_val, y_val_pred, target_names=['No Link', 'Link']))\n","\n","\n","# FINAL TEST EVALUATION\n","print(f\"\\n=== FINAL TEST EVALUATION ===\")\n","\n","\n","y_test_pred_proba = mlp.predict_proba(X_test_scaled)[:, 1]\n","y_test_pred = mlp.predict(X_test_scaled)\n","\n","test_loss = log_loss(y_test, y_test_pred_proba)\n","test_accuracy = accuracy_score(y_test, y_test_pred)\n","\n","print(f\"Final Test Log Loss: {test_loss:.4f}\")\n","print(f\"Final Test Accuracy: {test_accuracy:.4f}\")"],"metadata":{"id":"BqYsACV94Rjd"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["In the next two blocks we try to find the best params using the hyperopt library for our **XGB and LGB Models**"],"metadata":{"id":"6_GqxwXuwc_Z"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"0f0h1zTTkwMd"},"outputs":[],"source":["############################################################################\n","##########TRY TO FIND THE BEST PARAMS FOR OUR XGB MODEL#####################\n","############################################################################\n","\n","from hyperopt import fmin, tpe, hp, Trials, STATUS_OK\n","from xgboost import XGBClassifier\n","from sklearn.metrics import log_loss\n","from sklearn.model_selection import train_test_split\n","import numpy as np\n","\n","\n","\n","# Define objective function\n","def objective(space):\n","    model = XGBClassifier(\n","        objective='binary:logistic',\n","        use_label_encoder=False,\n","        eval_metric='logloss',\n","        tree_method='gpu_hist',\n","        gpu_id=0,\n","        n_estimators=space['n_estimators'],\n","        max_depth=space['max_depth'],\n","        learning_rate=space['learning_rate'],\n","        subsample=space['subsample'],\n","        colsample_bytree=space['colsample_bytree'],\n","        gamma=space['gamma'],\n","        min_child_weight=space['min_child_weight']\n","        # reg_alpha=space['reg_alpha'],\n","        # reg_lambda=space['reg_lambda']\n","    )\n","\n","\n","    cv_scores = cross_val_score(\n","        model, X_train_scaled, y_train,\n","        cv=3,  # 3-fold CV\n","        scoring='neg_log_loss',\n","        n_jobs=-1\n","    )\n","\n","\n","    loss = -cv_scores.mean()\n","    return {'loss': loss, 'status': STATUS_OK}\n","\n","# Define Hyperopt search space\n","search_space = {\n","    'n_estimators': hp.choice('n_estimators', list(range(20, 205, 5))),\n","    'max_depth': hp.choice('max_depth', list(range(5, 30))),\n","    'learning_rate': hp.quniform('learning_rate', 0.01, 0.5, 0.01),\n","    'subsample': hp.quniform('subsample', 0.1, 1, 0.01),\n","    'colsample_bytree': hp.quniform('colsample_bytree', 0.1, 1.0, 0.01),\n","    'gamma': hp.quniform('gamma', 0, 0.50, 0.01),\n","    'min_child_weight': hp.quniform('min_child_weight', 1, 10, 1)\n","}\n","\n","# Run optimization\n","trials = Trials()\n","best_params = fmin(fn=objective,\n","                   space=search_space,\n","                   algo=tpe.suggest,\n","                   max_evals=100,\n","                   trials=trials,\n","                   rstate=np.random.default_rng(42)\n","                  )\n","\n","print(\"Best hyperparameters from Hyperopt:\", best_params)\n","\n","best_params_actual = {\n","    'n_estimators': list(range(20, 205, 5))[best_params['n_estimators']],\n","    'max_depth': list(range(5, 30))[best_params['max_depth']],\n","    'learning_rate': best_params['learning_rate'],\n","    'subsample': best_params['subsample'],\n","    'colsample_bytree': best_params['colsample_bytree'],\n","    'gamma': best_params['gamma'],\n","    'min_child_weight': int(best_params['min_child_weight'])\n","}\n","\n","print(\"Best hyperparameters (actual values):\", best_params_actual)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8yEI5rxSb8by"},"outputs":[],"source":["from hyperopt import hp, fmin, tpe, STATUS_OK, Trials\n","from lightgbm import LGBMClassifier\n","from sklearn.metrics import log_loss\n","import numpy as np\n","import warnings\n","warnings.filterwarnings(\"ignore\")\n","\n","def objective(space):\n","    model = LGBMClassifier(\n","        n_estimators=int(space['n_estimators']),\n","        max_depth=int(space['max_depth']),\n","        learning_rate=space['learning_rate'],\n","        min_child_weight=space['min_child_weight'],\n","        num_leaves=int(space['num_leaves']),\n","        min_child_samples=int(space['min_child_samples']),\n","        subsample=space['subsample'],\n","        colsample_bytree=space['colsample_bytree'],\n","        reg_alpha=space['reg_alpha'],\n","        reg_lambda=space['reg_lambda'],\n","        random_state=42,\n","        n_jobs=-1,\n","        device='gpu',\n","        gpu_platform_id=0,\n","        gpu_device_id=0,\n","        verbose=-1\n","    )\n","\n","\n","    cv_scores = cross_val_score(\n","        model, X_train_scaled, y_train,\n","        cv=3,  # 3-fold CV\n","        scoring='neg_log_loss',\n","        n_jobs=1\n","    )\n","\n","    # Return positive log loss (lower is better)\n","    loss = -cv_scores.mean()\n","    return {'loss': loss, 'status': STATUS_OK}\n","\n","\n","search_space = {\n","    'n_estimators': hp.choice('n_estimators', [100, 200, 500, 1000]),\n","    'max_depth': hp.choice('max_depth', list(np.arange(5, 16, 1))),\n","    'learning_rate': hp.choice('learning_rate', list(np.arange(0.05, 0.31, 0.05))),\n","    'num_leaves': hp.quniform('num_leaves', 20, 50, 1),\n","    'min_child_samples': hp.quniform('min_child_samples', 5, 100, 1),\n","    'min_child_weight': hp.choice('min_child_weight', list(np.arange(1, 8, 1))),\n","    'subsample': hp.uniform('subsample', 0.8, 1.0),\n","    'colsample_bytree': hp.uniform('colsample_bytree', 0.3, 0.8),\n","    'reg_alpha': hp.uniform('reg_alpha', 0.0, 1.0),\n","    'reg_lambda': hp.uniform('reg_lambda', 0.0, 1.0)\n","}\n","\n","trials = Trials()\n","\n","best = fmin(\n","    fn=objective,\n","    space=search_space,\n","    algo=tpe.suggest,\n","    max_evals=50,\n","    trials=trials,\n","    rstate=np.random.default_rng(42)\n",")\n","\n","print(\"Best hyperparameters:\", best)\n"]},{"cell_type":"markdown","source":["Here we also do a **SHAP Analysis** for our top models from **XGB and LGB**"],"metadata":{"id":"luu-IN-Ww7pn"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"isQWt6-vEYjN"},"outputs":[],"source":["!pip install shap"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xklK5Lz4Eav7"},"outputs":[],"source":["import shap"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VCwnSocwEcHg"},"outputs":[],"source":["# Create TreeExplainer object\n","explainer_xgb = shap.TreeExplainer(bst)\n","\n","# Calculate SHAP values for validation data\n","shap_values_xgb = explainer_xgb.shap_values(X_val_scaled)\n","\n","# Summarize the effects of all the features\n","shap.summary_plot(shap_values_xgb, X_val_scaled, feature_names= [\n","                                                                  'cluster_sum', 'cluster_diff',\n","                                                                  'rank_sum', 'rank_diff',\n","                                                                  'h_sum', 'h_diff',\n","                                                                  'a_sum', 'a_diff',\n","                                                                  'bet_sum', 'bet_diff',\n","                                                                  'katz_sum', 'katz_diff'\n","                                                                ])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Kiqu9aizEjrX"},"outputs":[],"source":["# Create TreeExplainer object\n","explainer_lgb = shap.TreeExplainer(lgb_model)\n","\n","# Calculate SHAP values for validation data\n","shap_values_lgb = explainer_lgb.shap_values(X_val_scaled)\n","\n","# Summarize the effects of all the features\n","shap.summary_plot(shap_values_lgb, X_val_scaled, feature_names=[\n","                                                                  'cluster_sum', 'cluster_diff',\n","                                                                  'rank_sum', 'rank_diff',\n","                                                                  'h_sum', 'h_diff',\n","                                                                  'a_sum', 'a_diff',\n","                                                                  'bet_sum', 'bet_diff',\n","                                                                  'katz_sum', 'katz_diff'\n","                                                                ])"]},{"cell_type":"markdown","source":["Function for Predictions for our test pairs **(Kaggle test pairs) **"],"metadata":{"id":"f8w8ESojxtRB"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"r5EtnZ4DwoCW"},"outputs":[],"source":["def predict_for_test_set(test_pairs, model, scaler):\n","    predictions = []\n","\n","\n","\n","    with tqdm(enumerate(test_pairs), desc=\"Test predictions\") as pbar:\n","\n","      for pair_id, (source, target) in pbar:\n","\n","          # max_sim, avg_sim, count_above = get_sentence_bert_features(source, target, sentence_bert_embeddings)\n","\n","          features = [\n","            get_text_similarity(source, target),\n","            get_author_overlap(source, target),\n","            get_common_neighbors(source, target),\n","            get_jaccard_coefficient(source, target),\n","            get_preferential_attachment(source, target),\n","            get_embedding_similarity(source, target, node_embeddings),\n","            get_bert_similarity(source, target, specter_bert_embeddings),\n","            cluster[source] + cluster[target],\n","            abs(cluster[source] - cluster[target]),\n","            rank[source] + rank[target],\n","            abs(rank[source] - rank[target]),\n","            h[source] + h[target],\n","            abs(h[source] - h[target]),\n","            a[source] + a[target],\n","            abs(a[source] - a[target]),\n","            bet[source] + bet[target],\n","            abs(bet[source] - bet[target]),\n","            katz_centrality[source] + katz_centrality[target],\n","            abs(katz_centrality[source] - katz_centrality[target]),\n","            calculate_adamic_adar(source, target),\n","            calculate_shortest_path(source, target)\n","            # max_sim,\n","            # avg_sim,\n","            # count_above,\n","\n","        ]\n","\n","          # Scale features\n","          features_scaled = scaler.transform([features])\n","\n","          # Predict probability\n","          probability = model.predict_proba(features_scaled)[0, 1]\n","\n","          predictions.append((pair_id, float(probability)))\n","\n","    return predictions\n"]},{"cell_type":"markdown","source":["Predictions For **XGB**"],"metadata":{"id":"oxqoZCLexMEn"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"milBLizlwoOJ"},"outputs":[],"source":["# Make predictions for test set\n","predictions_bst = predict_for_test_set(test_pairs_kaggle, bst, scaler)\n","\n","# Create submission file\n","submission = pd.DataFrame(predictions_bst, columns=[\"ID\", \"Label\"])\n","submission.to_csv(\"/content/drive/MyDrive/llms/bst_new_feat.csv\", index=False)\n","\n","#tracker.stop()"]},{"cell_type":"markdown","source":["Prediction For **LGB**"],"metadata":{"id":"VCGRweudxOJ_"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"37pTNv_vOXfb"},"outputs":[],"source":["# Make predictions for test set\n","predictions_lgb = predict_for_test_set(test_pairs_kaggle, lgb_model, scaler)\n","\n","# Create submission file\n","submission = pd.DataFrame(predictions_lgb, columns=[\"ID\", \"Label\"])\n","submission.to_csv(\"/content/drive/MyDrive/llms/lgb_new_feat.csv\", index=False)"]},{"cell_type":"markdown","source":["**Combine The Two Models Predictions**"],"metadata":{"id":"-dB9jdfvsdQK"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"weILzSHKDPtu"},"outputs":[],"source":["import pandas as pd\n","\n","\n","predictions_bst_df = pd.DataFrame(predictions_bst, columns=[\"ID\", \"Label_bst\"])\n","predictions_lgb_df = pd.DataFrame(predictions_lgb, columns=[\"ID\", \"Label_lgb\"])\n","\n","\n","combined_predictions = pd.merge(predictions_bst_df, predictions_lgb_df, on=\"ID\")\n","\n","# Calculate the average prediction\n","combined_predictions[\"Label\"] = 0.5 * combined_predictions[\"Label_bst\"] + 0.5 * combined_predictions[\"Label_lgb\"]\n","\n","# Extract the final predictions as a list of tuples\n","final_preds = list(zip(combined_predictions[\"ID\"], combined_predictions[\"Label\"]))\n","\n","\n","submission = pd.DataFrame(final_preds, columns=[\"ID\", \"Label\"])\n","submission.to_csv(\"/content/drive/MyDrive/llms/combine_new_feats.csv\", index=False)"]},{"cell_type":"markdown","source":["Predictions For **Stack Model(Meta Model)**"],"metadata":{"id":"kRIm_bJWxa2T"}},{"cell_type":"code","source":["from sklearn.ensemble import StackingClassifier\n","from xgboost import XGBClassifier\n","from lightgbm import LGBMClassifier\n","from sklearn.linear_model import LogisticRegression\n","\n","stacked_model = StackingClassifier(\n","    estimators=[\n","        ('xgb', XGBClassifier()),\n","        ('lgb', LGBMClassifier()),\n","        ('logistic', LogisticRegression())\n","    ],\n","    final_estimator=LogisticRegression()\n",")\n","\n","stacked_model.fit(X_train_scaled, y_train)\n","\n","predictions_stack_model = predict_for_test_set(test_pairs_kaggle, stacked_model, scaler, node_embeddings)\n","\n","# Create submission file\n","submission = pd.DataFrame(predictions_lgb, columns=[\"ID\", \"Label\"])\n","submission.to_csv(\"/content/drive/MyDrive/llms/stacked_model_new_feats.csv\", index=False)\n"],"metadata":{"id":"lrVGdep3kxnF"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"L4","machine_shape":"hm","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.11"}},"nbformat":4,"nbformat_minor":0}