{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyMwNKOMwOfs/jSt0Jt5fLgE"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["import warnings\n","warnings.filterwarnings(\"ignore\")"],"metadata":{"id":"jcWp1EQytjh9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import locale\n","locale.getpreferredencoding = lambda: \"UTF-8\""],"metadata":{"id":"ZEfkodiNsfaF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pip install -U node2vec gensim"],"metadata":{"id":"mNQ_U6hmoFNw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from node2vec import Node2Vec\n","from sklearn.model_selection import train_test_split\n","import networkx as nx\n","import random\n","import numpy as np"],"metadata":{"id":"ZKQ5nAQzsf2V"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"gQzCBy4WryGi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Load citation network\n","edges = []\n","with open(\"/content/drive/MyDrive/llms/edgelist.txt\", \"r\") as f:\n","    for line in f:\n","        source, target = map(int, line.strip().split(\",\"))\n","        edges.append((source, target))\n","\n","# Create a directed graph\n","G = nx.DiGraph()\n","G.add_edges_from(edges)\n","print(G.number_of_nodes())\n","print(G.number_of_edges())"],"metadata":{"id":"Sf_vPch4r0Y1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# # Load test pairs\n","test_pairs_kaggle = []\n","with open(\"/content/drive/MyDrive/llms/test.txt\", \"r\") as f:\n","    for line in f:\n","        source, target = map(int, line.strip().split(\",\"))\n","        test_pairs_kaggle.append((source, target))"],"metadata":{"id":"QxJWE3XLr3qs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["random_state=42\n","random.seed(random_state)\n","np.random.seed(random_state)\n","\n","test_ratio=0.80\n","val_ratio=0.10\n","test_ratio = 0.10\n","\n","# Get all edges from original graph\n","all_edges = list(G.edges())\n","all_nodes = list(G.nodes())\n","\n","print(f\"Original graph - Nodes: {len(all_nodes)}, Edges: {len(all_edges)}\")\n","\n","# Split edges into test and train_val\n","train_val_edges, test_edges = train_test_split(\n","    all_edges,\n","    test_size=test_ratio,\n","    random_state=random_state\n",")\n","\n","# Do this in order to have 80% training edges, 10% val and 10% test\n","val_size_relative_to_train_val = val_ratio / (1.0 - test_ratio)\n","\n","train_edges, val_edges = train_test_split(\n","    train_val_edges,\n","    test_size=val_size_relative_to_train_val,\n","    random_state=random_state\n",")\n","\n","print(f\"Train edges: {len(train_edges)} ({len(train_edges)/len(all_edges)*100:.1f}%)\")\n","print(f\"Val edges: {len(val_edges)} ({len(val_edges)/len(all_edges)*100:.1f}%)\")\n","print(f\"Test edges: {len(test_edges)} ({len(test_edges)/len(all_edges)*100:.1f}%)\")\n","\n","# Create training graph\n","G_train = nx.DiGraph() if G.is_directed() else nx.Graph()\n","G_train.add_edges_from(train_edges)\n","G_train.add_nodes_from(all_nodes)\n","\n","print(f\"Training graph - Nodes: {G_train.number_of_nodes()}, Edges: {G_train.number_of_edges()}\")"],"metadata":{"id":"1yxfPHYhr67z"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# settings of node2vec\n","node2vec = Node2Vec(G_train, dimensions=64, walk_length=100, num_walks=10, workers=1)\n","\n","# other settings for node2vec\n","# node2vec = Node2Vec(G_train, dimensions=128, walk_length=30, num_walks=200, workers=4)"],"metadata":{"id":"PwwFFojxsZjT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Embed nodes\n","model = node2vec.fit(window=10, min_count=1, batch_words=100)"],"metadata":{"id":"drEqO4TJs6xp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Create a dictionary of node embeddings\n","node_embeddings = {}\n","for node in G_train.nodes():\n","  node_embeddings[node] = model.wv[str(node)]"],"metadata":{"id":"gC0OShVus8bk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pickle\n","\n","# Define the filename for the saved embeddings\n","embeddings_filename = \"/content/drive/MyDrive/llms/node_embeddings.pkl\"\n","\n","# Save the dictionary to a file\n","with open(embeddings_filename, 'wb') as f:\n","    pickle.dump(node_embeddings, f)\n","\n","print(f\"Node embeddings saved to {embeddings_filename}\")"],"metadata":{"id":"Wwner1YWsSgl"},"execution_count":null,"outputs":[]}]}